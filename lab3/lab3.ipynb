{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE 379K Lab 3\n",
    "## Rohan Nagar and Wenyang Fu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def jaccard_similarity(x, y, z):\n",
    "#     intersection_cardinality = len(set.intersection(*[set(x), set(y), set(z)]))\n",
    "#     union_cardinality = len(set.union(*[set(x), set(y), set(z)]))\n",
    "#     return intersection_cardinality / float(union_cardinality)\n",
    "\n",
    "# jaccard_similarity(['nike', 'running', 'shoe'],\n",
    "#                    ['nike', 'black', 'running', 'shoe'],\n",
    "#                    ['nike', 'blue', 'jacket', 'adidas'])\n",
    "\n",
    "def jaccard_similarity(x, y, z):\n",
    "    intersection_cardinality = len(x & y & z)\n",
    "    union_cardinality = len(x | y | z)\n",
    "    return intersection_cardinality / float(union_cardinality)\n",
    "\n",
    "jaccard_similarity({'nike', 'running', 'shoe'},\n",
    "                   {'nike', 'black', 'running', 'shoe'},\n",
    "                   {'nike', 'blue', 'jacket', 'adidas'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Minhash\n",
    "\n",
    "### Part A: Create the characteristic matrix with the alphabet as the seven words {'nike', 'running', 'shoe', 'black', 'blue', 'jacket', 'adidas'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nike' '1' '1' '1']\n",
      " ['running' '1' '1' '0']\n",
      " ['shoe' '1' '1' '0']\n",
      " ['black' '0' '1' '0']\n",
      " ['blue' '0' '0' '1']\n",
      " ['jacket' '0' '0' '1']\n",
      " ['adidas' '0' '0' '1']]\n"
     ]
    }
   ],
   "source": [
    "char_matrix = np.array([['nike',1,1,1],\n",
    "                         ['running',1,1,0],\n",
    "                         ['shoe',1,1,0],\n",
    "                         ['black',0,1,0],\n",
    "                         ['blue',0,0,1],\n",
    "                         ['jacket',0,0,1],\n",
    "                         ['adidas',0,0,1]])\n",
    "print(char_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: For a random permutation of the seven alphabet elements, find a way to compute the first non-zero element of each column (each set) under the permutation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nike', 'nike', 'adidas']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def first_elements(permutation):\n",
    "    first = []\n",
    "    for column in permutation.T:\n",
    "        for i, elem in enumerate(column):\n",
    "            if elem == '1':\n",
    "                first.append(permutation.T[0][i])\n",
    "                break\n",
    "    return first\n",
    "    \n",
    "permutation = np.random.permutation(char_matrix)\n",
    "first_elements(permutation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Instead of choosing a random permutation, use the hash function $h(x) = 3x + 2 (mod 7)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 5 1 4 0 3 6]\n",
      "[['shoe' '1' '1' '0']\n",
      " ['jacket' '0' '0' '1']\n",
      " ['running' '1' '1' '0']\n",
      " ['blue' '0' '0' '1']\n",
      " ['nike' '1' '1' '1']\n",
      " ['black' '0' '1' '0']\n",
      " ['adidas' '0' '0' '1']]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['shoe', 'shoe', 'jacket']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hash_func(x):\n",
    "    return (3 * x + 2) % 7\n",
    "\n",
    "hashf = np.vectorize(hash_func) # Vectorized function for hardware-parallel computation\n",
    "\n",
    "def hashed_permutation(char_matrix):\n",
    "    num_sets = char_matrix.shape[0]\n",
    "    idxs = hashf(np.arange(num_sets))\n",
    "    print(idxs)\n",
    "    return char_matrix[idxs]\n",
    "\n",
    "hashed_matrix = hashed_permutation(char_matrix)\n",
    "print(hashed_matrix)\n",
    "print(type(hashed_matrix))\n",
    "\n",
    "first_elements(hashed_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D: Generate your own hash functions of the form $h(x) = ax + b (mod 7)$ by choosing $a$ and $b$ at random from the set $\\{0, 1, ..., 6\\}$. Do this twenty times to estimate the Jaccard Similarity of the three sets. How closely do you approximate the true values, computed in question one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3571428571428573\n"
     ]
    }
   ],
   "source": [
    "choices = np.arange(7)\n",
    "s1, s2, s3 = (  {'nike', 'running', 'shoe'},\n",
    "                {'nike', 'black', 'running', 'shoe'},\n",
    "                {'nike', 'blue', 'jacket', 'adidas'}\n",
    "             )\n",
    "alphabet = ['nike', 'running', 'shoe', 'black', 'blue', 'jacket', 'adidas']\n",
    "\n",
    "def gen_hash_func(choices):\n",
    "    a = np.random.choice(choices)\n",
    "    b = np.random.choice(choices)\n",
    "\n",
    "    hash_func = lambda x: (a * x + b) % 7\n",
    "    return hash_func\n",
    "\n",
    "hash_funcs = [gen_hash_func(choices) for _ in range(20)]\n",
    "alphabet_dict = dict(zip(alphabet, range(len(alphabet)))) # Integer mapping of the alphabet.\n",
    "\n",
    "s1 = {alphabet_dict[elem] for elem in s1}\n",
    "s2 = {alphabet_dict[elem] for elem in s2}\n",
    "s3 = {alphabet_dict[elem] for elem in s3}\n",
    "\n",
    "sets = (s1, s2, s3) # Tuple of 3 sets\n",
    "\n",
    "def jac_sim_experiment(sets, hashf):\n",
    "    hashed = [{hashf(e) for e in s}\n",
    "                        for s in sets]\n",
    "    \n",
    "    jaccard_score = jaccard_similarity(hashed[0], hashed[1], hashed[2])\n",
    "    return jaccard_score\n",
    "\n",
    "experimental_avg = sum([jac_sim_experiment(sets, f) for f in hash_funcs]) / 20\n",
    "print(experimental_avg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the experiment a few times, the experimental score ranges anywhere from perfect (1/7) to around .48. So its usually not too accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Implementing Minhash\n",
    "\n",
    "### Repeat the above where instead of permuting the entire characteristic matrix, implement the algorithm described in Chapter 3 of MMDS for implementing Minhash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.,  0.,  1.],\n",
       "       [ 4.,  3.,  0.],\n",
       "       [ 2.,  0.,  1.],\n",
       "       [ 6.,  6.,  6.],\n",
       "       [ 6.,  6.,  6.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def minhash(a, hash_funcs):\n",
    "    \"\"\" This implementation of MinHash\n",
    "    will take a 2-dimensional characteristic matrix \n",
    "    and return the hashed signature matrix.\n",
    "    \n",
    "    a - 2-dim ndarray \n",
    "    hash_funcs - set of hash functions to use\n",
    "    \"\"\"\n",
    "    alphabet_size = a.shape[0]\n",
    "    num_sets = a.shape[1]\n",
    "    n = len(hash_funcs)\n",
    "    \n",
    "    sig = np.zeros((n, num_sets)) # preallocate Signature matrix\n",
    "    sig.fill(np.inf)\n",
    "    \n",
    "    for r in range(alphabet_size):\n",
    "        row_hashes = np.array([hash_funcs[i](r) for i in range(n)])\n",
    "        col_idxs = np.where(a[r, :] == '1')[0] # Columns to fill in\n",
    "        num_idxs = len(col_idxs)\n",
    "        sig[:, col_idxs] = np.minimum(sig[:, col_idxs], \n",
    "                                      np.array([row_hashes] * num_idxs).T)\n",
    "    return sig\n",
    "\n",
    "char_matrix = np.array([['nike',1,1,1],\n",
    "                         ['running',1,1,0],\n",
    "                         ['shoe',1,1,0],\n",
    "                         ['black',0,1,0],\n",
    "                         ['blue',0,0,1],\n",
    "                         ['jacket',0,0,1],\n",
    "                         ['adidas',0,0,1]])\n",
    "sig = minhash(char_matrix[:, 1:] ,hash_funcs[:5])\n",
    "\n",
    "sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rows, num_cols = sig.shape\n",
    "\n",
    "sets = [set(sig[i, :]) for i in range(num_cols)]\n",
    "jaccard_score = len(set.intersection(*sets)) / len(set.union(*sets))\n",
    "jaccard_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: More Minhash (Shingling)\n",
    "\n",
    "### Part A: Load the 5 article excerpts in $\\tt{Lab3articles-5.txt}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t120 The Supreme Court in Johnnesberg on Friday postponed until March 14 a hearing on a petition by government minister Winnie Mandela to prevent police reading documents seized from her home, the SAPA news agency reported. David Wright and Carlos Delgado homered and Jorge Sosa won for the sixth time as the New York Mets snapped a four-game losing streak with a 3-0 victory over Detroit on Friday night. US Defense Secretary Robert Gates said on Sunday that Iran was not yet able to make a nuclear weapon and that its program was progressing slower than Tehran expected. A Palestinian suicide bomber blew himself up in a crowded hotel dining room here Wednesday evening just as more than 200 people gathered for their Passover holiday meal, killing at least 19 and wounding more than 100 others, many of them children. OPEC kingpin Saudi Arabia signalled Tuesday it could act alone to meet a predicted increase in demand for oil, as it pushed hesitant fellow members of the cartel to raise production quotas immediately. The most important form of political compromise in Iraq isn't among top Iraqi politicians in Baghdad, but at the local level, President Bush asserted Thursday, in a departure from past rhetoric on Iraqi politics. Nancy Smith likes the HBO series ``Sex and the City,'' but the high school drama teacher often can't be in front of the television on Sunday nights, when new episodes air. An exhibition opened here today to market the 50th anniversary of China sending its volunteers to join forces with the People's Army of the Democratic People's Republic of Korea (DPRK) in the War to Resist U.S. Aggression and Aid Korea (1950-53).\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "with open('Lab3articles-5.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        documents.append(line.strip())\n",
    "    \n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Use $\\tt{stopwords}$ from $\\tt{nltk.corpus}$ to strip the stopwords from the 5 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t120', 'The', 'Supreme', 'Court', 'Johnnesberg', 'Friday', 'postponed', 'March', '14', 'hearing', 'petition', 'government', 'minister', 'Winnie', 'Mandela', 'prevent', 'police', 'reading', 'documents', 'seized', 'home,', 'SAPA', 'news', 'agency', 'reported.', 'David', 'Wright', 'Carlos', 'Delgado', 'homered', 'Jorge', 'Sosa', 'sixth', 'time', 'New', 'York', 'Mets', 'snapped', 'four-game', 'losing', 'streak', '3-0', 'victory', 'Detroit', 'Friday', 'night.', 'US', 'Defense', 'Secretary', 'Robert', 'Gates', 'said', 'Sunday', 'Iran', 'yet', 'able', 'make', 'nuclear', 'weapon', 'program', 'progressing', 'slower', 'Tehran', 'expected.', 'A', 'Palestinian', 'suicide', 'bomber', 'blew', 'crowded', 'hotel', 'dining', 'room', 'Wednesday', 'evening', '200', 'people', 'gathered', 'Passover', 'holiday', 'meal,', 'killing', 'least', '19', 'wounding', '100', 'others,', 'many', 'children.', 'OPEC', 'kingpin', 'Saudi', 'Arabia', 'signalled', 'Tuesday', 'could', 'act', 'alone', 'meet', 'predicted', 'increase', 'demand', 'oil,', 'pushed', 'hesitant', 'fellow', 'members', 'cartel', 'raise', 'production', 'quotas', 'immediately.', 'The', 'important', 'form', 'political', 'compromise', 'Iraq', \"isn't\", 'among', 'top', 'Iraqi', 'politicians', 'Baghdad,', 'local', 'level,', 'President', 'Bush', 'asserted', 'Thursday,', 'departure', 'past', 'rhetoric', 'Iraqi', 'politics.', 'Nancy', 'Smith', 'likes', 'HBO', 'series', '``Sex', \"City,''\", 'high', 'school', 'drama', 'teacher', 'often', \"can't\", 'front', 'television', 'Sunday', 'nights,', 'new', 'episodes', 'air.', 'An', 'exhibition', 'opened', 'today', 'market', '50th', 'anniversary', 'China', 'sending', 'volunteers', 'join', 'forces', \"People's\", 'Army', 'Democratic', \"People's\", 'Republic', 'Korea', '(DPRK)', 'War', 'Resist', 'U.S.', 'Aggression', 'Aid', 'Korea', '(1950-53).']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "stripped_documents = []\n",
    "for document in documents:\n",
    "    excerpt = [word for word in document.split() if word not in stop_words]\n",
    "    stripped_documents.append(excerpt)\n",
    "    \n",
    "print(stripped_documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Compute the $k$-shingles of the documents for $k = 2$, where you shingle on the words, not letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t120 The', 'The Supreme', 'Supreme Court', 'Court Johnnesberg', 'Johnnesberg Friday', 'Friday postponed', 'postponed March', 'March 14', '14 hearing', 'hearing petition', 'petition government', 'government minister', 'minister Winnie', 'Winnie Mandela', 'Mandela prevent', 'prevent police', 'police reading', 'reading documents', 'documents seized', 'seized home,', 'home, SAPA', 'SAPA news', 'news agency', 'agency reported.', 'reported. David', 'David Wright', 'Wright Carlos', 'Carlos Delgado', 'Delgado homered', 'homered Jorge', 'Jorge Sosa', 'Sosa sixth', 'sixth time', 'time New', 'New York', 'York Mets', 'Mets snapped', 'snapped four-game', 'four-game losing', 'losing streak', 'streak 3-0', '3-0 victory', 'victory Detroit', 'Detroit Friday', 'Friday night.', 'night. US', 'US Defense', 'Defense Secretary', 'Secretary Robert', 'Robert Gates', 'Gates said', 'said Sunday', 'Sunday Iran', 'Iran yet', 'yet able', 'able make', 'make nuclear', 'nuclear weapon', 'weapon program', 'program progressing', 'progressing slower', 'slower Tehran', 'Tehran expected.', 'expected. A', 'A Palestinian', 'Palestinian suicide', 'suicide bomber', 'bomber blew', 'blew crowded', 'crowded hotel', 'hotel dining', 'dining room', 'room Wednesday', 'Wednesday evening', 'evening 200', '200 people', 'people gathered', 'gathered Passover', 'Passover holiday', 'holiday meal,', 'meal, killing', 'killing least', 'least 19', '19 wounding', 'wounding 100', '100 others,', 'others, many', 'many children.', 'children. OPEC', 'OPEC kingpin', 'kingpin Saudi', 'Saudi Arabia', 'Arabia signalled', 'signalled Tuesday', 'Tuesday could', 'could act', 'act alone', 'alone meet', 'meet predicted', 'predicted increase', 'increase demand', 'demand oil,', 'oil, pushed', 'pushed hesitant', 'hesitant fellow', 'fellow members', 'members cartel', 'cartel raise', 'raise production', 'production quotas', 'quotas immediately.', 'immediately. The', 'The important', 'important form', 'form political', 'political compromise', 'compromise Iraq', \"Iraq isn't\", \"isn't among\", 'among top', 'top Iraqi', 'Iraqi politicians', 'politicians Baghdad,', 'Baghdad, local', 'local level,', 'level, President', 'President Bush', 'Bush asserted', 'asserted Thursday,', 'Thursday, departure', 'departure past', 'past rhetoric', 'rhetoric Iraqi', 'Iraqi politics.', 'politics. Nancy', 'Nancy Smith', 'Smith likes', 'likes HBO', 'HBO series', 'series ``Sex', \"``Sex City,''\", \"City,'' high\", 'high school', 'school drama', 'drama teacher', 'teacher often', \"often can't\", \"can't front\", 'front television', 'television Sunday', 'Sunday nights,', 'nights, new', 'new episodes', 'episodes air.', 'air. An', 'An exhibition', 'exhibition opened', 'opened today', 'today market', 'market 50th', '50th anniversary', 'anniversary China', 'China sending', 'sending volunteers', 'volunteers join', 'join forces', \"forces People's\", \"People's Army\", 'Army Democratic', \"Democratic People's\", \"People's Republic\", 'Republic Korea', 'Korea (DPRK)', '(DPRK) War', 'War Resist', 'Resist U.S.', 'U.S. Aggression', 'Aggression Aid', 'Aid Korea']\n"
     ]
    }
   ],
   "source": [
    "def k_shingles_words(documents, k=2):\n",
    "    all_shingles = []\n",
    "    for document in documents:\n",
    "        shingles = []\n",
    "        size = len(document)\n",
    "        for i, word in enumerate(document):\n",
    "            if i >= size - k: break\n",
    "            for j in range(i+1, i+k):\n",
    "                word += (' ' + document[j])\n",
    "            shingles.append(word)\n",
    "            \n",
    "        all_shingles.append(shingles)\n",
    "        \n",
    "    return all_shingles\n",
    "        \n",
    "print(k_shingles_words(stripped_documents)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D: Compute the $k$-shingles of the documents for $k = 3$, where you shingle on the characters, not words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t12', '120', '20 ', '0 T', ' Th', 'The', 'he ', 'e S', ' Su', 'Sup', 'upr', 'pre', 'rem', 'eme', 'me ', 'e C', ' Co', 'Cou', 'our', 'urt', 'rt ', 't J', ' Jo', 'Joh', 'ohn', 'hnn', 'nne', 'nes', 'esb', 'sbe', 'ber', 'erg', 'rg ', 'g F', ' Fr', 'Fri', 'rid', 'ida', 'day', 'ay ', 'y p', ' po', 'pos', 'ost', 'stp', 'tpo', 'pon', 'one', 'ned', 'ed ', 'd M', ' Ma', 'Mar', 'arc', 'rch', 'ch ', 'h 1', ' 14', '14 ', '4 h', ' he', 'hea', 'ear', 'ari', 'rin', 'ing', 'ng ', 'g p', ' pe', 'pet', 'eti', 'tit', 'iti', 'tio', 'ion', 'on ', 'n g', ' go', 'gov', 'ove', 'ver', 'ern', 'rnm', 'nme', 'men', 'ent', 'nt ', 't m', ' mi', 'min', 'ini', 'nis', 'ist', 'ste', 'ter', 'er ', 'r W', ' Wi', 'Win', 'inn', 'nni', 'nie', 'ie ', 'e M', ' Ma', 'Man', 'and', 'nde', 'del', 'ela', 'la ', 'a p', ' pr', 'pre', 'rev', 'eve', 'ven', 'ent', 'nt ', 't p', ' po', 'pol', 'oli', 'lic', 'ice', 'ce ', 'e r', ' re', 'rea', 'ead', 'adi', 'din', 'ing', 'ng ', 'g d', ' do', 'doc', 'ocu', 'cum', 'ume', 'men', 'ent', 'nts', 'ts ', 's s', ' se', 'sei', 'eiz', 'ize', 'zed', 'ed ', 'd h', ' ho', 'hom', 'ome', 'me,', 'e, ', ', S', ' SA', 'SAP', 'APA', 'PA ', 'A n', ' ne', 'new', 'ews', 'ws ', 's a', ' ag', 'age', 'gen', 'enc', 'ncy', 'cy ', 'y r', ' re', 'rep', 'epo', 'por', 'ort', 'rte', 'ted', 'ed.', 'd. ', '. D', ' Da', 'Dav', 'avi', 'vid', 'id ', 'd W', ' Wr', 'Wri', 'rig', 'igh', 'ght', 'ht ', 't C', ' Ca', 'Car', 'arl', 'rlo', 'los', 'os ', 's D', ' De', 'Del', 'elg', 'lga', 'gad', 'ado', 'do ', 'o h', ' ho', 'hom', 'ome', 'mer', 'ere', 'red', 'ed ', 'd J', ' Jo', 'Jor', 'org', 'rge', 'ge ', 'e S', ' So', 'Sos', 'osa', 'sa ', 'a s', ' si', 'six', 'ixt', 'xth', 'th ', 'h t', ' ti', 'tim', 'ime', 'me ', 'e N', ' Ne', 'New', 'ew ', 'w Y', ' Yo', 'Yor', 'ork', 'rk ', 'k M', ' Me', 'Met', 'ets', 'ts ', 's s', ' sn', 'sna', 'nap', 'app', 'ppe', 'ped', 'ed ', 'd f', ' fo', 'fou', 'our', 'ur-', 'r-g', '-ga', 'gam', 'ame', 'me ', 'e l', ' lo', 'los', 'osi', 'sin', 'ing', 'ng ', 'g s', ' st', 'str', 'tre', 'rea', 'eak', 'ak ', 'k 3', ' 3-', '3-0', '-0 ', '0 v', ' vi', 'vic', 'ict', 'cto', 'tor', 'ory', 'ry ', 'y D', ' De', 'Det', 'etr', 'tro', 'roi', 'oit', 'it ', 't F', ' Fr', 'Fri', 'rid', 'ida', 'day', 'ay ', 'y n', ' ni', 'nig', 'igh', 'ght', 'ht.', 't. ', '. U', ' US', 'US ', 'S D', ' De', 'Def', 'efe', 'fen', 'ens', 'nse', 'se ', 'e S', ' Se', 'Sec', 'ecr', 'cre', 'ret', 'eta', 'tar', 'ary', 'ry ', 'y R', ' Ro', 'Rob', 'obe', 'ber', 'ert', 'rt ', 't G', ' Ga', 'Gat', 'ate', 'tes', 'es ', 's s', ' sa', 'sai', 'aid', 'id ', 'd S', ' Su', 'Sun', 'und', 'nda', 'day', 'ay ', 'y I', ' Ir', 'Ira', 'ran', 'an ', 'n y', ' ye', 'yet', 'et ', 't a', ' ab', 'abl', 'ble', 'le ', 'e m', ' ma', 'mak', 'ake', 'ke ', 'e n', ' nu', 'nuc', 'ucl', 'cle', 'lea', 'ear', 'ar ', 'r w', ' we', 'wea', 'eap', 'apo', 'pon', 'on ', 'n p', ' pr', 'pro', 'rog', 'ogr', 'gra', 'ram', 'am ', 'm p', ' pr', 'pro', 'rog', 'ogr', 'gre', 'res', 'ess', 'ssi', 'sin', 'ing', 'ng ', 'g s', ' sl', 'slo', 'low', 'owe', 'wer', 'er ', 'r T', ' Te', 'Teh', 'ehr', 'hra', 'ran', 'an ', 'n e', ' ex', 'exp', 'xpe', 'pec', 'ect', 'cte', 'ted', 'ed.', 'd. ', '. A', ' A ', 'A P', ' Pa', 'Pal', 'ale', 'les', 'est', 'sti', 'tin', 'ini', 'nia', 'ian', 'an ', 'n s', ' su', 'sui', 'uic', 'ici', 'cid', 'ide', 'de ', 'e b', ' bo', 'bom', 'omb', 'mbe', 'ber', 'er ', 'r b', ' bl', 'ble', 'lew', 'ew ', 'w c', ' cr', 'cro', 'row', 'owd', 'wde', 'ded', 'ed ', 'd h', ' ho', 'hot', 'ote', 'tel', 'el ', 'l d', ' di', 'din', 'ini', 'nin', 'ing', 'ng ', 'g r', ' ro', 'roo', 'oom', 'om ', 'm W', ' We', 'Wed', 'edn', 'dne', 'nes', 'esd', 'sda', 'day', 'ay ', 'y e', ' ev', 'eve', 'ven', 'eni', 'nin', 'ing', 'ng ', 'g 2', ' 20', '200', '00 ', '0 p', ' pe', 'peo', 'eop', 'opl', 'ple', 'le ', 'e g', ' ga', 'gat', 'ath', 'the', 'her', 'ere', 'red', 'ed ', 'd P', ' Pa', 'Pas', 'ass', 'sso', 'sov', 'ove', 'ver', 'er ', 'r h', ' ho', 'hol', 'oli', 'lid', 'ida', 'day', 'ay ', 'y m', ' me', 'mea', 'eal', 'al,', 'l, ', ', k', ' ki', 'kil', 'ill', 'lli', 'lin', 'ing', 'ng ', 'g l', ' le', 'lea', 'eas', 'ast', 'st ', 't 1', ' 19', '19 ', '9 w', ' wo', 'wou', 'oun', 'und', 'ndi', 'din', 'ing', 'ng ', 'g 1', ' 10', '100', '00 ', '0 o', ' ot', 'oth', 'the', 'her', 'ers', 'rs,', 's, ', ', m', ' ma', 'man', 'any', 'ny ', 'y c', ' ch', 'chi', 'hil', 'ild', 'ldr', 'dre', 'ren', 'en.', 'n. ', '. O', ' OP', 'OPE', 'PEC', 'EC ', 'C k', ' ki', 'kin', 'ing', 'ngp', 'gpi', 'pin', 'in ', 'n S', ' Sa', 'Sau', 'aud', 'udi', 'di ', 'i A', ' Ar', 'Ara', 'rab', 'abi', 'bia', 'ia ', 'a s', ' si', 'sig', 'ign', 'gna', 'nal', 'all', 'lle', 'led', 'ed ', 'd T', ' Tu', 'Tue', 'ues', 'esd', 'sda', 'day', 'ay ', 'y c', ' co', 'cou', 'oul', 'uld', 'ld ', 'd a', ' ac', 'act', 'ct ', 't a', ' al', 'alo', 'lon', 'one', 'ne ', 'e m', ' me', 'mee', 'eet', 'et ', 't p', ' pr', 'pre', 'red', 'edi', 'dic', 'ict', 'cte', 'ted', 'ed ', 'd i', ' in', 'inc', 'ncr', 'cre', 'rea', 'eas', 'ase', 'se ', 'e d', ' de', 'dem', 'ema', 'man', 'and', 'nd ', 'd o', ' oi', 'oil', 'il,', 'l, ', ', p', ' pu', 'pus', 'ush', 'she', 'hed', 'ed ', 'd h', ' he', 'hes', 'esi', 'sit', 'ita', 'tan', 'ant', 'nt ', 't f', ' fe', 'fel', 'ell', 'llo', 'low', 'ow ', 'w m', ' me', 'mem', 'emb', 'mbe', 'ber', 'ers', 'rs ', 's c', ' ca', 'car', 'art', 'rte', 'tel', 'el ', 'l r', ' ra', 'rai', 'ais', 'ise', 'se ', 'e p', ' pr', 'pro', 'rod', 'odu', 'duc', 'uct', 'cti', 'tio', 'ion', 'on ', 'n q', ' qu', 'quo', 'uot', 'ota', 'tas', 'as ', 's i', ' im', 'imm', 'mme', 'med', 'edi', 'dia', 'iat', 'ate', 'tel', 'ely', 'ly.', 'y. ', '. T', ' Th', 'The', 'he ', 'e i', ' im', 'imp', 'mpo', 'por', 'ort', 'rta', 'tan', 'ant', 'nt ', 't f', ' fo', 'for', 'orm', 'rm ', 'm p', ' po', 'pol', 'oli', 'lit', 'iti', 'tic', 'ica', 'cal', 'al ', 'l c', ' co', 'com', 'omp', 'mpr', 'pro', 'rom', 'omi', 'mis', 'ise', 'se ', 'e I', ' Ir', 'Ira', 'raq', 'aq ', 'q i', ' is', 'isn', \"sn'\", \"n't\", \"'t \", 't a', ' am', 'amo', 'mon', 'ong', 'ng ', 'g t', ' to', 'top', 'op ', 'p I', ' Ir', 'Ira', 'raq', 'aqi', 'qi ', 'i p', ' po', 'pol', 'oli', 'lit', 'iti', 'tic', 'ici', 'cia', 'ian', 'ans', 'ns ', 's B', ' Ba', 'Bag', 'agh', 'ghd', 'hda', 'dad', 'ad,', 'd, ', ', l', ' lo', 'loc', 'oca', 'cal', 'al ', 'l l', ' le', 'lev', 'eve', 'vel', 'el,', 'l, ', ', P', ' Pr', 'Pre', 'res', 'esi', 'sid', 'ide', 'den', 'ent', 'nt ', 't B', ' Bu', 'Bus', 'ush', 'sh ', 'h a', ' as', 'ass', 'sse', 'ser', 'ert', 'rte', 'ted', 'ed ', 'd T', ' Th', 'Thu', 'hur', 'urs', 'rsd', 'sda', 'day', 'ay,', 'y, ', ', d', ' de', 'dep', 'epa', 'par', 'art', 'rtu', 'tur', 'ure', 're ', 'e p', ' pa', 'pas', 'ast', 'st ', 't r', ' rh', 'rhe', 'het', 'eto', 'tor', 'ori', 'ric', 'ic ', 'c I', ' Ir', 'Ira', 'raq', 'aqi', 'qi ', 'i p', ' po', 'pol', 'oli', 'lit', 'iti', 'tic', 'ics', 'cs.', 's. ', '. N', ' Na', 'Nan', 'anc', 'ncy', 'cy ', 'y S', ' Sm', 'Smi', 'mit', 'ith', 'th ', 'h l', ' li', 'lik', 'ike', 'kes', 'es ', 's H', ' HB', 'HBO', 'BO ', 'O s', ' se', 'ser', 'eri', 'rie', 'ies', 'es ', 's `', ' ``', '``S', '`Se', 'Sex', 'ex ', 'x C', ' Ci', 'Cit', 'ity', 'ty,', \"y,'\", \",''\", \"'' \", \"' h\", ' hi', 'hig', 'igh', 'gh ', 'h s', ' sc', 'sch', 'cho', 'hoo', 'ool', 'ol ', 'l d', ' dr', 'dra', 'ram', 'ama', 'ma ', 'a t', ' te', 'tea', 'eac', 'ach', 'che', 'her', 'er ', 'r o', ' of', 'oft', 'fte', 'ten', 'en ', 'n c', ' ca', 'can', \"an'\", \"n't\", \"'t \", 't f', ' fr', 'fro', 'ron', 'ont', 'nt ', 't t', ' te', 'tel', 'ele', 'lev', 'evi', 'vis', 'isi', 'sio', 'ion', 'on ', 'n S', ' Su', 'Sun', 'und', 'nda', 'day', 'ay ', 'y n', ' ni', 'nig', 'igh', 'ght', 'hts', 'ts,', 's, ', ', n', ' ne', 'new', 'ew ', 'w e', ' ep', 'epi', 'pis', 'iso', 'sod', 'ode', 'des', 'es ', 's a', ' ai', 'air', 'ir.', 'r. ', '. A', ' An', 'An ', 'n e', ' ex', 'exh', 'xhi', 'hib', 'ibi', 'bit', 'iti', 'tio', 'ion', 'on ', 'n o', ' op', 'ope', 'pen', 'ene', 'ned', 'ed ', 'd t', ' to', 'tod', 'oda', 'day', 'ay ', 'y m', ' ma', 'mar', 'ark', 'rke', 'ket', 'et ', 't 5', ' 50', '50t', '0th', 'th ', 'h a', ' an', 'ann', 'nni', 'niv', 'ive', 'ver', 'ers', 'rsa', 'sar', 'ary', 'ry ', 'y C', ' Ch', 'Chi', 'hin', 'ina', 'na ', 'a s', ' se', 'sen', 'end', 'ndi', 'din', 'ing', 'ng ', 'g v', ' vo', 'vol', 'olu', 'lun', 'unt', 'nte', 'tee', 'eer', 'ers', 'rs ', 's j', ' jo', 'joi', 'oin', 'in ', 'n f', ' fo', 'for', 'orc', 'rce', 'ces', 'es ', 's P', ' Pe', 'Peo', 'eop', 'opl', 'ple', \"le'\", \"e's\", \"'s \", 's A', ' Ar', 'Arm', 'rmy', 'my ', 'y D', ' De', 'Dem', 'emo', 'moc', 'ocr', 'cra', 'rat', 'ati', 'tic', 'ic ', 'c P', ' Pe', 'Peo', 'eop', 'opl', 'ple', \"le'\", \"e's\", \"'s \", 's R', ' Re', 'Rep', 'epu', 'pub', 'ubl', 'bli', 'lic', 'ic ', 'c K', ' Ko', 'Kor', 'ore', 'rea', 'ea ', 'a (', ' (D', '(DP', 'DPR', 'PRK', 'RK)', 'K) ', ') W', ' Wa', 'War', 'ar ', 'r R', ' Re', 'Res', 'esi', 'sis', 'ist', 'st ', 't U', ' U.', 'U.S', '.S.', 'S. ', '. A', ' Ag', 'Agg', 'ggr', 'gre', 'res', 'ess', 'ssi', 'sio', 'ion', 'on ', 'n A', ' Ai', 'Aid', 'id ', 'd K', ' Ko', 'Kor', 'ore', 'rea', 'ea ', 'a (', ' (1', '(19', '195', '950', '50-', '0-5', '-53', '53)']\n"
     ]
    }
   ],
   "source": [
    "def k_shingles_characters(documents, k=3):\n",
    "    all_shingles = []\n",
    "    for doc in documents:\n",
    "        shingles = []\n",
    "        size = len(doc)\n",
    "        for i in range(size-k):\n",
    "            shingles.append(doc[i:i+k])\n",
    "        all_shingles.append(shingles)\n",
    "        \n",
    "    return all_shingles\n",
    "\n",
    "# It's easy to remove spaces - simply join the document on the empty string instead of on ' '.\n",
    "joined = [' '.join(document) for document in stripped_documents]\n",
    "print(k_shingles_characters(joined)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Even More Minhash (Document Similarity)\n",
    "\n",
    "### Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.63147800e+06   9.33698200e+06   1.12120910e+07   1.32671390e+07\n",
      "    3.51558390e+07   2.63147800e+06]\n",
      " [  3.51215800e+06   4.52623830e+07   6.26854800e+07   4.90393730e+07\n",
      "    1.31122530e+07   2.85068190e+07]\n",
      " [  4.38651510e+07   6.25279660e+07   7.47984300e+06   5.46072200e+06\n",
      "    1.12457895e+08   2.28763290e+07]\n",
      " [  1.58465480e+07   1.96667530e+07   6.69884200e+06   7.52103390e+07\n",
      "    3.09209570e+07   8.18601800e+06]\n",
      " [  3.99302620e+07   3.77920330e+07   1.20998600e+07   1.70909390e+07\n",
      "    3.02720660e+07   3.99302620e+07]\n",
      " [  3.35240500e+06   6.82206000e+05   2.16914630e+07   2.47228320e+07\n",
      "    1.13560780e+07   3.56633450e+07]\n",
      " [  7.77004500e+06   5.20203460e+07   2.37938940e+07   3.94202880e+07\n",
      "    1.71163600e+07   2.50254700e+06]\n",
      " [  7.26045000e+06   1.75660200e+07   6.68082670e+07   4.36638140e+07\n",
      "    3.96510880e+07   7.26045000e+06]\n",
      " [  1.39954430e+07   2.30123910e+07   1.72469320e+07   8.52987900e+06\n",
      "    5.66208100e+06   1.39954430e+07]\n",
      " [  9.54256000e+05   4.91562600e+07   2.29187440e+07   6.59334800e+06\n",
      "    1.28235060e+07   2.51679030e+07]\n",
      " [  5.99104240e+07   1.78523870e+07   1.41891690e+07   1.89239560e+07\n",
      "    3.26951300e+06   7.03731380e+07]\n",
      " [  2.03494550e+07   4.32806100e+07   4.80074500e+06   1.22098470e+07\n",
      "    5.41521300e+06   5.41521300e+06]\n",
      " [  4.68741100e+06   1.82589900e+06   7.84857800e+06   3.12885130e+07\n",
      "    4.37362440e+07   4.68741100e+06]\n",
      " [  3.28504730e+07   2.95913850e+07   1.39029080e+07   9.44875800e+06\n",
      "    3.11072940e+07   2.94372120e+07]\n",
      " [  6.64846900e+06   1.58155390e+07   9.67153800e+06   8.18730700e+06\n",
      "    6.66883740e+07   6.64846900e+06]\n",
      " [  2.27830260e+07   3.60061700e+06   3.18140040e+07   9.07422700e+06\n",
      "    8.53061000e+05   4.69105570e+07]\n",
      " [  3.31134900e+06   2.24091990e+07   2.04846570e+07   2.04846570e+07\n",
      "    2.27782790e+07   3.31134900e+06]\n",
      " [  5.17826400e+06   2.17850250e+07   1.69426400e+07   5.74584200e+07\n",
      "    3.04858020e+07   7.86563300e+06]\n",
      " [  1.04915280e+07   5.59257020e+07   1.01911500e+06   6.06182300e+06\n",
      "    1.86573720e+07   6.41999100e+06]\n",
      " [  8.10682000e+05   7.90886500e+06   1.75154610e+07   2.13840310e+07\n",
      "    9.98312400e+06   8.10682000e+05]\n",
      " [  1.14461090e+07   4.05629220e+07   1.38437590e+07   2.39822370e+07\n",
      "    2.85178900e+06   1.14461090e+07]\n",
      " [  3.13621630e+07   2.18072260e+07   5.22980600e+06   1.76597200e+06\n",
      "    4.81308160e+07   1.47662610e+07]\n",
      " [  1.72343190e+07   9.76385700e+06   2.23979000e+06   1.35191170e+07\n",
      "    1.30905070e+07   1.80329250e+07]\n",
      " [  9.16320000e+04   1.14144900e+07   4.77143410e+07   7.21039600e+06\n",
      "    3.28942200e+07   9.16320000e+04]\n",
      " [  1.44770670e+07   1.09020690e+07   8.18559200e+06   2.32192100e+06\n",
      "    4.09684930e+07   3.82649200e+07]\n",
      " [  4.50316970e+07   7.22891150e+07   9.36581700e+06   1.16206040e+07\n",
      "    9.08952500e+07   4.50316970e+07]\n",
      " [  6.70611800e+06   1.01480790e+07   2.83961330e+07   7.10730950e+07\n",
      "    2.35875620e+07   6.70611800e+06]\n",
      " [  1.29101710e+07   4.35626000e+06   3.16445660e+07   2.13228350e+07\n",
      "    5.50581000e+05   1.51231663e+08]\n",
      " [  3.50374620e+07   1.34898490e+07   4.20872860e+07   3.38869620e+07\n",
      "    9.69875000e+05   3.50374620e+07]\n",
      " [  1.05696000e+05   1.45298800e+07   2.84165300e+06   5.58860000e+06\n",
      "    3.17959060e+07   1.05696000e+05]]\n"
     ]
    }
   ],
   "source": [
    "from binascii import crc32\n",
    "import sys\n",
    "\n",
    "p = 4294967311\n",
    "n = 30\n",
    "\n",
    "# Word shingles\n",
    "word_signature_matrix = np.ndarray(shape=(n,len(stripped_documents)))\n",
    "word_signature_matrix.fill(sys.maxsize)\n",
    "for i in range(n):\n",
    "    a, b = np.random.randint(p-1), np.random.randint(p-1)\n",
    "    word_shingles = k_shingles_words(stripped_documents)\n",
    "    \n",
    "    for idx, document in enumerate(word_shingles):\n",
    "        signatures = []\n",
    "        for word in document:\n",
    "            x = crc32(str.encode(word))\n",
    "            value = (a*x + b) % p\n",
    "            \n",
    "            if not word_signature_matrix[i][idx] < value:\n",
    "                word_signature_matrix[i][idx] = value\n",
    "    \n",
    "print(word_signature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6182352.    790596.   8540733.   2422151.  14411189.   9843478.]\n",
      " [ 11232138.   6181187.   1259303.  23389440.   1250030.  20354874.]\n",
      " [  1931344.   1931344.  22144456.   1606993.  24903416.   1931344.]\n",
      " [  1978934.   1978934.   2416765.   7186557.   2416765.   2416765.]\n",
      " [ 14358863.   2853049.    401586.  15936466.   7480201.   2853049.]\n",
      " [  7115388.   4761501.  13872642.   9024131.  13639651.  13872642.]\n",
      " [  7726852.   2927640.   2927640.  10734508.    212493.   7726852.]\n",
      " [  3148650.    410046.   7037274.   7481045.   7995123.   3148650.]\n",
      " [  3826156.   6213664.   1961236.   5590085.   1669069.   5590085.]\n",
      " [  5108539.   1146631.   8230792.   6067650.   1146631.   1146631.]\n",
      " [  3522639.    142375.    594954.   5758633.   1312424.    142375.]\n",
      " [   760157.    760157.   7583270.    233556.   4451546.    760157.]\n",
      " [  7047114.   7047114.   5539247.   5539247.   6844141.   7047114.]\n",
      " [  9539518.   4874017.   4874017.    632447.    824242.   3761109.]\n",
      " [  7333839.    580615.  18006797.  10385746.   7333839.   7333839.]\n",
      " [   981615.   9174571.   4611767.  11403773.   8069950.    981615.]\n",
      " [  5256881.   5256881.    858151.   5256881.   5256881.   5256881.]\n",
      " [  7036122.    104824.   7036122.    551684.   2939341.   7036122.]\n",
      " [  4421972.   3986008.   3514192.   4421972.   1175293.   4421972.]\n",
      " [ 16682278.    195224.    195224.    195224.   8469149.   5617854.]\n",
      " [  6804152.   2226633.   2226633.   3720197.   6120859.   3720197.]\n",
      " [   698387.  11504314.  11935495.   8441158.   4440134.    698387.]\n",
      " [  4270510.   4558252.   6721595.   6721595.   6721595.   4558252.]\n",
      " [   811537.    811537.   1938277.   1938277.   1938277.    811537.]\n",
      " [  9496787.   2807686.   5655546.   5655546.    652776.  14064310.]\n",
      " [  3231490.   3231490.   3231490.   4482645.   4093772.   3231490.]\n",
      " [  3401722.   3943362.  10495618.   3943362.   3943362.   3401722.]\n",
      " [  1559649.   1559649.  13756082.   1559649.   1559649.   1559649.]\n",
      " [ 10092120.   2628761.   2444413.  10259043.   2444413.  10092120.]\n",
      " [  5480830.   5799149.   5480830.   3331156.   2404611.   5480830.]]\n"
     ]
    }
   ],
   "source": [
    "# Character shingles\n",
    "char_signature_matrix = np.ndarray(shape=(n,len(stripped_documents)))\n",
    "char_signature_matrix.fill(sys.maxsize)\n",
    "for i in range(n):\n",
    "    a, b = np.random.randint(p-1), np.random.randint(p-1)\n",
    "    char_shingles = k_shingles_characters(joined)\n",
    "    \n",
    "    for idx, document in enumerate(char_shingles):\n",
    "        signatures = []\n",
    "        for word in document:\n",
    "            x = crc32(str.encode(word))\n",
    "            value = (a*x + b) % p\n",
    "            \n",
    "            if not char_signature_matrix[i][idx] < value:\n",
    "                char_signature_matrix[i][idx] = value\n",
    "    \n",
    "print(char_signature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All we need to do is compare the column 0 to all other columns. The estimated similarity is the number of\n",
    "# same values over the total number of values (30)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
