{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE 379K - Data Science Lab\n",
    "# Lab 9\n",
    "# Wenyang Fu and Rohan Nagar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import (cross_val_score, train_test_split,\n",
    "                                    GridSearchCV, RandomizedSearchCV)\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_inclass = pd.read_csv('data/train_inclass.csv')\n",
    "test_inclass = pd.read_csv('data/test_inclass.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    49998.000000\n",
      "mean         5.272668\n",
      "std        224.530270\n",
      "min         -0.372758\n",
      "25%          0.038775\n",
      "50%          0.186073\n",
      "75%          0.563830\n",
      "max      29110.040580\n",
      "Name: F3, dtype: float64\n",
      "\n",
      "count    49998.000000\n",
      "mean         5.273124\n",
      "std        224.529521\n",
      "min          0.000000\n",
      "25%          0.030389\n",
      "50%          0.154672\n",
      "75%          0.555344\n",
      "max      29110.000000\n",
      "Name: F23, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(train_inclass['F3'].describe())\n",
    "print()\n",
    "print(train_inclass['F23'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of added noise: 0.07956681166550796\n",
      "Variance of added noise: 0.003602256845676487\n"
     ]
    }
   ],
   "source": [
    "difference = abs(train_inclass['F23'] - train_inclass['F3'])\n",
    "\n",
    "print('Mean of added noise: {}'.format(difference.mean()))\n",
    "print('Variance of added noise: {}'.format(difference.std()**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 \n",
    "As we explained in lecture, the InClass competition data came from  https://www.kaggle.com/c/GiveMeSomeCredit\n",
    "\n",
    "You can now double the training data and you have a new validation set using the leaderboard of this Kaggle competition.\n",
    "\n",
    "You can also look at `Data Dictionary.xls' to find what each of the features are exactly.\n",
    "\n",
    "Train your models on the additional data and validate using the private LB of that competition. How do the optimal hyperparameters parameters change ? Are the winning XGB parameters still better?\n",
    "Report your Private LB score and include a screenshot of your submissions in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_preds(filename, preds):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write('Id,Probability\\n')\n",
    "        for num, pred in zip(range(1,101504), preds):\n",
    "            f.write('{},{}\\n'.format(num, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/cs-training.csv', index_col=0)\n",
    "test = pd.read_csv('data/cs-test.csv', index_col=0)\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop dependent variable in test\n",
    "test = test.drop(['SeriousDlqin2yrs'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fill missing with mean\n",
    "train = train.fillna(train.mean())\n",
    "test = test.fillna(test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Seperate dependent and independent\n",
    "X_train = train.drop(['SeriousDlqin2yrs'], axis=1)\n",
    "y_train = train['SeriousDlqin2yrs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Perform a log transform on the data\n",
    "transformer = FunctionTransformer(np.log1p)\n",
    "X_train = transformer.transform(X_train)\n",
    "test = transformer.transform(test)\n",
    "X_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohannagar/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# XGB, Raymond Wen's parameters\n",
    "# Raymond Wen's parameters\n",
    "params = {\n",
    "    'n_estimators': 1000,\n",
    "    'eta': 0.01,\n",
    "    'max_depth': 4,\n",
    "    'min_child_weight': 5,\n",
    "    'subsample': 0.4,\n",
    "    'gamma': 0.8,\n",
    "    'colsample_bytree': 0.4,\n",
    "    'lambda': 0.93,\n",
    "    'alpha': 0.5,\n",
    "    'eval_metric': 'auc',\n",
    "    'objective': 'binary:logistic',\n",
    "    # Increase this number if you have more cores.\n",
    "    # Otherwise, remove it and it will default\n",
    "    # to the maxium number.\n",
    "    'nthread': 4,\n",
    "    'booster': 'gbtree',\n",
    "    'tree_method': 'exact',\n",
    "    'silent': 1,\n",
    "    'seed': SEED\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.747949+0.0466865\ttest-auc:0.744769+0.0521322\n",
      "[500]\ttrain-auc:0.868464+0.000509831\ttest-auc:0.864688+0.00180149\n"
     ]
    }
   ],
   "source": [
    "# check model CV scores\n",
    "num_boost_round = int(params['n_estimators'])\n",
    "del params['n_estimators']\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "score_history = xgb.cv(params, dtrain, num_boost_round,\n",
    "                           nfold=5, stratified=True,\n",
    "                           early_stopping_rounds=250,\n",
    "                           verbose_eval=500)\n",
    "\n",
    "# Only use scores from the final boosting round since that's the one\n",
    "# that performed the best.\n",
    "mean_final_round = score_history.tail(1).iloc[0, 0]\n",
    "std_final_round = score_history.tail(1).iloc[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMean Score: 0.8663118000000001\n",
      "\n",
      "\tStd Dev: 0.0018126794973187925\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\tMean Score: {0}\\n\".format(mean_final_round))\n",
    "print(\"\\tStd Dev: {0}\\n\\n\".format(std_final_round))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As of version 0.6, XGBoost returns a dataframe of the following form:\n",
    "# boosting iter | mean_test_err | mean_test_std | mean_train_err | mean_train_std\n",
    "# boost iter 1 mean_test_iter1 | mean_test_std1 | ... | ...\n",
    "# boost iter 2 mean_test_iter2 | mean_test_std2 | ... | ...\n",
    "# ...\n",
    "# boost iter n_estimators\n",
    "\n",
    "xg_booster = xgb.train(params, dtrain, num_boost_round)\n",
    "preds = xg_booster.predict(dtest)\n",
    "write_preds('submissions/xgb_raymond_{}.csv'.format(SEED), preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raymond's hyperparameters achieved a private leaderboard score of $\t0.867641$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "# Let OpenMP use 4 threads to evaluate models - may run into errors\n",
    "# if this is not set. Should be set before hyperopt import.\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "import hyperopt\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=\"logs/hyperopt_xgb.log\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.771441+0.0304771\ttest-auc:0.766047+0.0346485\n",
      "[500]\ttrain-auc:0.876783+0.000469591\ttest-auc:0.866306+0.00182939\n",
      "[0]\ttrain-auc:0.7669+0.0394836\ttest-auc:0.76161+0.0419847\n",
      "[0]\ttrain-auc:0.762717+0.0412081\ttest-auc:0.759824+0.0446267\n",
      "[0]\ttrain-auc:0.773133+0.0366549\ttest-auc:0.764708+0.0403129\n",
      "[0]\ttrain-auc:0.778639+0.0288682\ttest-auc:0.771273+0.0324843\n",
      "[500]\ttrain-auc:0.900989+0.000451365\ttest-auc:0.86548+0.00206645\n",
      "[0]\ttrain-auc:0.758615+0.0436854\ttest-auc:0.755917+0.0460673\n",
      "[0]\ttrain-auc:0.778721+0.0329956\ttest-auc:0.761332+0.0414229\n",
      "[0]\ttrain-auc:0.800929+0.025089\ttest-auc:0.730082+0.0452533\n",
      "[0]\ttrain-auc:0.756091+0.0431604\ttest-auc:0.752678+0.0475648\n",
      "[0]\ttrain-auc:0.769693+0.030857\ttest-auc:0.764447+0.0348932\n",
      "[500]\ttrain-auc:0.875552+0.000536211\ttest-auc:0.866435+0.00188262\n",
      "[0]\ttrain-auc:0.787605+0.03528\ttest-auc:0.768254+0.0419516\n",
      "[0]\ttrain-auc:0.776661+0.0376994\ttest-auc:0.767846+0.0428384\n",
      "[0]\ttrain-auc:0.755086+0.0446986\ttest-auc:0.752432+0.0469662\n",
      "[500]\ttrain-auc:0.881635+0.000709419\ttest-auc:0.866173+0.00224314\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.763539+0.0435745\ttest-auc:0.758604+0.0463404\n",
      "[500]\ttrain-auc:0.882679+0.0007133\ttest-auc:0.866585+0.0018844\n",
      "[0]\ttrain-auc:0.775558+0.0285237\ttest-auc:0.770177+0.0317029\n",
      "[0]\ttrain-auc:0.77713+0.0284574\ttest-auc:0.771147+0.0322218\n",
      "[500]\ttrain-auc:0.893024+0.000602726\ttest-auc:0.865761+0.00234567\n",
      "[0]\ttrain-auc:0.785937+0.0361956\ttest-auc:0.764466+0.04228\n",
      "[0]\ttrain-auc:0.752636+0.0421576\ttest-auc:0.749351+0.0472248\n",
      "[0]\ttrain-auc:0.752061+0.044355\ttest-auc:0.749117+0.0466121\n",
      "[0]\ttrain-auc:0.76853+0.0407507\ttest-auc:0.761894+0.0452379\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.760751+0.0435294\ttest-auc:0.757672+0.0475081\n",
      "[500]\ttrain-auc:0.882013+0.000677385\ttest-auc:0.86647+0.0018747\n",
      "[0]\ttrain-auc:0.76303+0.0411598\ttest-auc:0.757849+0.0445284\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.761165+0.0431798\ttest-auc:0.757668+0.0474485\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.768132+0.0389637\ttest-auc:0.758342+0.046246\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.760511+0.0433667\ttest-auc:0.75783+0.047517\n",
      "[500]\ttrain-auc:0.882074+0.000706871\ttest-auc:0.866504+0.00191711\n",
      "[0]\ttrain-auc:0.767084+0.0402917\ttest-auc:0.761037+0.0440676\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.755598+0.0439696\ttest-auc:0.752513+0.0471887\n",
      "[500]\ttrain-auc:0.875667+0.000537976\ttest-auc:0.866289+0.00189119\n",
      "[1000]\ttrain-auc:0.881881+0.000601151\ttest-auc:0.866363+0.00209384\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.770782+0.0374382\ttest-auc:0.764947+0.0409292\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.778953+0.0297636\ttest-auc:0.771732+0.0336069\n",
      "[500]\ttrain-auc:0.882128+0.000577257\ttest-auc:0.866511+0.00202866\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.763505+0.043589\ttest-auc:0.758493+0.0462579\n",
      "[500]\ttrain-auc:0.881775+0.000600932\ttest-auc:0.866575+0.00193755\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.763505+0.043589\ttest-auc:0.758493+0.0462579\n",
      "[500]\ttrain-auc:0.881775+0.000600932\ttest-auc:0.866575+0.00193755\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.754393+0.0445669\ttest-auc:0.752324+0.0478946\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.780127+0.0349547\ttest-auc:0.765172+0.0407941\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.76311+0.0434524\ttest-auc:0.758846+0.0464165\n",
      "[500]\ttrain-auc:0.882042+0.000661047\ttest-auc:0.866633+0.00186998\n",
      "[0]\ttrain-auc:0.760719+0.0436015\ttest-auc:0.757942+0.047639\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.762684+0.0411832\ttest-auc:0.759936+0.0446773\n",
      "[0]\ttrain-auc:0.763108+0.0434704\ttest-auc:0.758842+0.046409\n",
      "[500]\ttrain-auc:0.882985+0.00066513\ttest-auc:0.866593+0.00187552\n",
      "[0]\ttrain-auc:0.763108+0.0434704\ttest-auc:0.758842+0.046409\n",
      "[500]\ttrain-auc:0.882985+0.00066513\ttest-auc:0.866593+0.00187552\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.763108+0.0434704\ttest-auc:0.758842+0.046409\n",
      "[500]\ttrain-auc:0.882985+0.00066513\ttest-auc:0.866593+0.00187552\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.763108+0.0434704\ttest-auc:0.758842+0.046409\n",
      "[500]\ttrain-auc:0.882985+0.00066513\ttest-auc:0.866593+0.00187552\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.763108+0.0434704\ttest-auc:0.758842+0.046409\n",
      "[500]\ttrain-auc:0.882985+0.00066513\ttest-auc:0.866593+0.00187552\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.777353+0.0358651\ttest-auc:0.766101+0.0407284\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.758245+0.0465703\ttest-auc:0.755692+0.0489982\n",
      "[500]\ttrain-auc:0.881623+0.000589681\ttest-auc:0.866511+0.00195397\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.760719+0.0436015\ttest-auc:0.757942+0.047639\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.759522+0.041753\ttest-auc:0.754926+0.0455971\n",
      "[500]\ttrain-auc:0.881847+0.000653877\ttest-auc:0.866436+0.00213884\n",
      "[0]\ttrain-auc:0.752594+0.0475628\ttest-auc:0.749975+0.0501285\n",
      "[500]\ttrain-auc:0.875307+0.000533532\ttest-auc:0.86638+0.00185262\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.763417+0.0438047\ttest-auc:0.759608+0.047114\n",
      "[500]\ttrain-auc:0.882645+0.000673949\ttest-auc:0.866432+0.00180865\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.754959+0.0439946\ttest-auc:0.75147+0.0472519\n",
      "[500]\ttrain-auc:0.87608+0.000573683\ttest-auc:0.866341+0.00181525\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.760555+0.0435942\ttest-auc:0.755476+0.0491714\n",
      "[0]\ttrain-auc:0.763474+0.0436002\ttest-auc:0.758573+0.0462787\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.754959+0.0439945\ttest-auc:0.751489+0.0472597\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.816456+0.0196411\ttest-auc:0.711439+0.0476554\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.754959+0.0439946\ttest-auc:0.75147+0.0472519\n",
      "[500]\ttrain-auc:0.875412+0.000609402\ttest-auc:0.866337+0.00188633\n",
      "[0]\ttrain-auc:0.761673+0.0439665\ttest-auc:0.758566+0.0462544\n",
      "[0]\ttrain-auc:0.75509+0.0419859\ttest-auc:0.751636+0.0461888\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.765478+0.0415685\ttest-auc:0.761118+0.0432056\n",
      "[500]\ttrain-auc:0.882771+0.000540355\ttest-auc:0.866557+0.00188783\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.776316+0.0361495\ttest-auc:0.766517+0.0410733\n",
      "[0]\ttrain-auc:0.763108+0.0434704\ttest-auc:0.758842+0.046409\n",
      "[0]\ttrain-auc:0.747235+0.0478879\ttest-auc:0.745245+0.0509949\n",
      "[500]\ttrain-auc:0.874397+0.000600934\ttest-auc:0.866306+0.00183929\n",
      "[1000]\ttrain-auc:0.880345+0.000644882\ttest-auc:0.866419+0.00201833\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.760103+0.0443194\ttest-auc:0.756384+0.0477751\n",
      "[0]\ttrain-auc:0.7631+0.043451\ttest-auc:0.758813+0.0464074\n",
      "[500]\ttrain-auc:0.881461+0.000629381\ttest-auc:0.866529+0.00196688\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.773724+0.0290227\ttest-auc:0.768064+0.0316364\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.751269+0.0455283\ttest-auc:0.749284+0.0488604\n",
      "[500]\ttrain-auc:0.881558+0.000518602\ttest-auc:0.866408+0.00227421\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.776262+0.036304\ttest-auc:0.76637+0.0407023\n",
      "[0]\ttrain-auc:0.768882+0.0383169\ttest-auc:0.762418+0.0411155\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.76659+0.038973\ttest-auc:0.761855+0.0416062\n",
      "[500]\ttrain-auc:0.895728+0.000718801\ttest-auc:0.866175+0.00228806\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.77373+0.0372875\ttest-auc:0.766155+0.0417738\n",
      "[500]\ttrain-auc:0.896431+0.000662105\ttest-auc:0.866047+0.00220631\n",
      "[0]\ttrain-auc:0.758738+0.0449404\ttest-auc:0.754872+0.0492701\n",
      "[500]\ttrain-auc:0.889977+0.000762277\ttest-auc:0.865467+0.00234209\n",
      "[0]\ttrain-auc:0.789471+0.0263721\ttest-auc:0.780179+0.0295967\n",
      "[0]\ttrain-auc:0.774275+0.037864\ttest-auc:0.766464+0.0418789\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.770836+0.0375671\ttest-auc:0.765024+0.0413325\n",
      "[500]\ttrain-auc:0.896191+0.000721614\ttest-auc:0.866064+0.00218483\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.766541+0.0390235\ttest-auc:0.761885+0.0415818\n",
      "[500]\ttrain-auc:0.895607+0.000706252\ttest-auc:0.866042+0.0022801\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.76659+0.038973\ttest-auc:0.761855+0.0416062\n",
      "[500]\ttrain-auc:0.895728+0.000718801\ttest-auc:0.866175+0.00228806\n",
      "[0]\ttrain-auc:0.774225+0.0372069\ttest-auc:0.765875+0.041614\n",
      "[500]\ttrain-auc:0.896547+0.000632156\ttest-auc:0.865968+0.00214934\n",
      "[0]\ttrain-auc:0.774201+0.0372002\ttest-auc:0.765942+0.0416204\n",
      "[500]\ttrain-auc:0.896483+0.000661713\ttest-auc:0.86611+0.00213591\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.774201+0.0372002\ttest-auc:0.765942+0.0416204\n",
      "[500]\ttrain-auc:0.896483+0.000661713\ttest-auc:0.86611+0.00213591\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.766406+0.0418873\ttest-auc:0.760552+0.0453045\n",
      "[500]\ttrain-auc:0.893464+0.000860053\ttest-auc:0.865694+0.00219457\n",
      "[0]\ttrain-auc:0.77373+0.0372875\ttest-auc:0.766155+0.0417738\n",
      "[500]\ttrain-auc:0.896431+0.000662105\ttest-auc:0.866047+0.00220631\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.747954+0.0441475\ttest-auc:0.745278+0.0475702\n",
      "[500]\ttrain-auc:0.874654+0.000629202\ttest-auc:0.866259+0.00202391\n",
      "[0]\ttrain-auc:0.767734+0.0406666\ttest-auc:0.762123+0.0451324\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.773061+0.0363113\ttest-auc:0.764742+0.0399449\n",
      "[0]\ttrain-auc:0.776687+0.0399863\ttest-auc:0.767415+0.0443809\n",
      "[500]\ttrain-auc:0.896375+0.000751826\ttest-auc:0.865717+0.00218438\n",
      "[0]\ttrain-auc:0.778824+0.0344754\ttest-auc:0.765758+0.0414967\n",
      "[0]\ttrain-auc:0.764368+0.0410277\ttest-auc:0.759834+0.0463478\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.77373+0.0372875\ttest-auc:0.766155+0.0417738\n",
      "[500]\ttrain-auc:0.896431+0.000662105\ttest-auc:0.866047+0.00220631\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.778511+0.0380653\ttest-auc:0.770153+0.0428036\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.776819+0.0361528\ttest-auc:0.766283+0.0409607\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.773555+0.0420757\ttest-auc:0.766221+0.045173\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.762061+0.0408762\ttest-auc:0.757106+0.0438786\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.755165+0.0438644\ttest-auc:0.751948+0.0470213\n",
      "[500]\ttrain-auc:0.87563+0.000577302\ttest-auc:0.866364+0.00190521\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.768373+0.0381835\ttest-auc:0.762412+0.0411692\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.776687+0.0399863\ttest-auc:0.767415+0.0443809\n",
      "[500]\ttrain-auc:0.896375+0.000751826\ttest-auc:0.865717+0.00218438\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.774146+0.0378243\ttest-auc:0.766361+0.0418834\n",
      "[500]\ttrain-auc:0.899157+0.000574355\ttest-auc:0.86601+0.00192838\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.762553+0.0436265\ttest-auc:0.758882+0.0457615\n",
      "[0]\ttrain-auc:0.76659+0.038973\ttest-auc:0.761855+0.0416062\n",
      "[500]\ttrain-auc:0.895728+0.000718801\ttest-auc:0.866175+0.00228806\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.752606+0.0448341\ttest-auc:0.749696+0.0475433\n",
      "[500]\ttrain-auc:0.881513+0.000527624\ttest-auc:0.866536+0.00236448\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.756175+0.0453363\ttest-auc:0.753169+0.0491988\n",
      "[500]\ttrain-auc:0.888698+0.000849572\ttest-auc:0.86578+0.00216512\n",
      "[0]\ttrain-auc:0.784403+0.0270008\ttest-auc:0.777012+0.0298264\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.768769+0.0402963\ttest-auc:0.764177+0.0429446\n",
      "[500]\ttrain-auc:0.894692+0.000866697\ttest-auc:0.865984+0.00221741\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.756495+0.0443989\ttest-auc:0.753335+0.0482404\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.782104+0.0284431\ttest-auc:0.775044+0.0315727\n",
      "[500]\ttrain-auc:0.89732+0.00061171\ttest-auc:0.865836+0.0020987\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.752607+0.0448322\ttest-auc:0.749696+0.0475436\n",
      "[500]\ttrain-auc:0.875448+0.000528686\ttest-auc:0.866347+0.00190901\n",
      "[1000]\ttrain-auc:0.88203+0.000555938\ttest-auc:0.866409+0.00212919\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.76659+0.038973\ttest-auc:0.761855+0.0416062\n",
      "[500]\ttrain-auc:0.895728+0.000718801\ttest-auc:0.866175+0.00228806\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.766612+0.0389781\ttest-auc:0.761812+0.0415919\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.752606+0.0448341\ttest-auc:0.749696+0.0475433\n",
      "[500]\ttrain-auc:0.875184+0.000583308\ttest-auc:0.866345+0.00188909\n",
      "[0]\ttrain-auc:0.76659+0.038973\ttest-auc:0.761855+0.0416062\n",
      "[500]\ttrain-auc:0.895728+0.000718801\ttest-auc:0.866175+0.00228806\n",
      "[0]\ttrain-auc:0.772757+0.0371411\ttest-auc:0.766254+0.0413289\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.76659+0.038973\ttest-auc:0.761855+0.0416062\n",
      "[500]\ttrain-auc:0.895728+0.000718801\ttest-auc:0.866175+0.00228806\n",
      "[0]\ttrain-auc:0.768767+0.040246\ttest-auc:0.764111+0.0427932\n",
      "[500]\ttrain-auc:0.896967+0.000725382\ttest-auc:0.865802+0.00214231\n",
      "[0]\ttrain-auc:0.768224+0.0383059\ttest-auc:0.762535+0.0410486\n",
      "[0]\ttrain-auc:0.761712+0.0438702\ttest-auc:0.758767+0.0462556\n",
      "[500]\ttrain-auc:0.894399+0.000927674\ttest-auc:0.865792+0.00213055\n",
      "[0]\ttrain-auc:0.768475+0.0383223\ttest-auc:0.762548+0.040908\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "[0]\ttrain-auc:0.754959+0.0439945\ttest-auc:0.751489+0.0472597\n",
      "[500]\ttrain-auc:0.875379+0.000597828\ttest-auc:0.866333+0.00186733\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "The best hyperparameters are:  \n",
      "\n",
      "{'subsample': 2, 'lambda': 2, 'max_depth': 2, 'min_child_weight': 2, 'gamma': 0, 'eta': 0.025, 'alpha': 3, 'colsample_bytree': 0.5, 'n_estimators': 1}\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "#                       HYPEROPT\n",
    "# -----------------------------------------------------\n",
    "\n",
    "def score(params):\n",
    "    logging.info(\"Training with params: \")\n",
    "    logging.info(params)\n",
    "    # Delete 'n_estimators' because it's only a constructor param\n",
    "    # when you're using  XGB's sklearn API.\n",
    "    # Instead, we have to save 'n_estimators' (# of boosting rounds)\n",
    "    # to xgb.cv().\n",
    "    num_boost_round = int(params['n_estimators'])\n",
    "    del params['n_estimators']\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    # As of version 0.6, XGBoost returns a dataframe of the following form:\n",
    "    # boosting iter | mean_test_err | mean_test_std | mean_train_err | mean_train_std\n",
    "    # boost iter 1 mean_test_iter1 | mean_test_std1 | ... | ...\n",
    "    # boost iter 2 mean_test_iter2 | mean_test_std2 | ... | ...\n",
    "    # ...\n",
    "    # boost iter n_estimators\n",
    "\n",
    "    score_history = xgb.cv(params, dtrain, num_boost_round,\n",
    "                           nfold=5, stratified=True,\n",
    "                           early_stopping_rounds=250,\n",
    "                           verbose_eval=500)\n",
    "    # Only use scores from the final boosting round since that's the one\n",
    "    # that performed the best.\n",
    "    mean_final_round = score_history.tail(1).iloc[0, 0]\n",
    "    std_final_round = score_history.tail(1).iloc[0, 1]\n",
    "    logging.info(\"\\tMean Score: {0}\\n\".format(mean_final_round))\n",
    "    logging.info(\"\\tStd Dev: {0}\\n\\n\".format(std_final_round))\n",
    "    # score() needs to return the loss (1 - score)\n",
    "    # since optimize() should be finding the minimum, and AUC\n",
    "    # naturally finds the maximum.\n",
    "    loss = 1 - mean_final_round\n",
    "    return {'loss': loss, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "def optimize(\n",
    "    # trials,\n",
    "        random_state=SEED):\n",
    "    \"\"\"\n",
    "    This is the optimization function that given a space (space here) of\n",
    "    hyperparameters and a scoring function (score here),\n",
    "    finds the best hyperparameters.\n",
    "    \"\"\"\n",
    "\n",
    "    space = {\n",
    "        'n_estimators': hp.choice('n_estimators', [1000, 1100]),\n",
    "        'eta': hp.quniform('eta', 0.01, 0.1, 0.025),\n",
    "        'max_depth': hp.choice('max_depth', [4, 5, 7, 9, 17]),\n",
    "        'min_child_weight': hp.choice('min_child_weight', [3, 5, 7]),\n",
    "        'subsample': hp.choice('subsample', [0.4, 0.6, 0.8]),\n",
    "        'gamma': hp.choice('gamma', [0.3, 0.4]),\n",
    "        'colsample_bytree': hp.quniform('colsample_bytree', 0.4, 0.7, 0.1),\n",
    "        'lambda': hp.choice('lambda', [0.01, 0.1, 0.9, 1.0]),\n",
    "        'alpha': hp.choice('alpha', [0, 0.1, 0.5, 1.0]),\n",
    "        'eval_metric': 'auc',\n",
    "        'objective': 'binary:logistic',\n",
    "        # Increase this number if you have more cores.\n",
    "        # Otherwise, remove it and it will default\n",
    "        # to the maxium number.\n",
    "        'nthread': 4,\n",
    "        'booster': 'gbtree',\n",
    "        'tree_method': 'exact',\n",
    "        'silent': 1,\n",
    "        'seed': random_state\n",
    "    }\n",
    "\n",
    "    # Use the fmin function from Hyperopt to find the best hyperparameters\n",
    "    best = fmin(score, space, algo=tpe.suggest,\n",
    "                # trials=trials,\n",
    "                max_evals=250)\n",
    "    return best\n",
    "\n",
    "\n",
    "best_hyperparams = optimize(\n",
    "    # trials\n",
    ")\n",
    "print(\"The best hyperparameters are: \", \"\\n\")\n",
    "print(best_hyperparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'silent': 1,\n",
    "    'seed': 42,\n",
    "    'subsample': 0.8,\n",
    "    'eta': 0.025,\n",
    "    'nthread': 4,\n",
    "    'eval_metric': 'auc',\n",
    "    'lambda': 0.9,\n",
    "    'booster': 'gbtree',\n",
    "    'alpha': 1.0,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 7,\n",
    "    'min_child_weight': 7,\n",
    "    'gamma': 0.3,\n",
    "    'tree_method': 'exact',\n",
    "    'n_estimators': 1100\n",
    "}\n",
    "\n",
    "# check model CV scores\n",
    "num_boost_round = int(params['n_estimators'])\n",
    "del params['n_estimators']\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "xg_booster = xgb.train(params, dtrain, num_boost_round)\n",
    "preds = xg_booster.predict(dtest)\n",
    "write_preds('submissions/xgb_mybest_{}.csv'.format(SEED), preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final private score for these hyperparams: $0.866054$, which is worse than Raymond's hyperparams. However, these obtained a higher CV score than Raymond's hyperparams. It's possible that we haven't discovered the optimal hyperparams for this dataset (didn't fully explore the search space), but that Raymond's parameters are still quite good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 \n",
    "\n",
    "`Data Dictionary.xls` explains that you are making your decisions on giving loans using the Total balance on credit cards, the Monthly debt payments, the number of mortgage loands of the individual etc. You are now asked to tell a story from this dataset.\n",
    "\n",
    "## Part A\n",
    "\n",
    "Fit a simple logistic regression model and report which features are important (and how they influence the deliquency chance). Discuss what is expected and what is surprising. See how regularization changes the importance of features.\n",
    "\n",
    "Would you expect that the number of dependents to have a postive or negative effect in deliquency probability? Discuss what you think and what the data says."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 10)\n",
      "(150000, 5)\n",
      "0.514343479402\n",
      "[ True  True  True False False False  True False  True False]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "log_clf = LogisticRegression().fit(X_train, y_train)\n",
    "model = SelectFromModel(log_clf, prefit=True)\n",
    "\n",
    "X_new = model.transform(X_train)\n",
    "print(X_train.shape)\n",
    "print(X_new.shape)\n",
    "print(model.threshold_)\n",
    "print(model.get_support())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features that were eliminated were:\n",
    "- DebtRatio\n",
    "- MonthlyIncome\n",
    "- NumberofOpenCreditLines\n",
    "- NumberofRealEstateLoansOrLines\n",
    "- NumberOfDependents.\n",
    "\n",
    "The features that were important were: \n",
    "- RevolvingUtilizationOfUnsecuredLines\n",
    "- age\n",
    "- NumberOfTime30-59DaysPastDueNotWorse\n",
    "- NumberOfTimes90DaysLate\n",
    "- NumberOfTime60-89DaysPastDueNotWorse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 10)\n",
      "(150000, 5)\n",
      "0.148917013943\n",
      "[ True  True  True False  True False  True False False False]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "log_clf = LogisticRegression(C=0.0001).fit(X_train, y_train)\n",
    "model = SelectFromModel(log_clf, prefit=True)\n",
    "\n",
    "X_new = model.transform(X_train)\n",
    "print(X_train.shape)\n",
    "print(X_new.shape)\n",
    "print(model.threshold_)\n",
    "print(model.get_support())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization may be changing the relative feature importances, but SelectFromModel consistently selects the same 5 features across C=1, 0.5, 0.1, 0.01, 0.001, and 0.0001. Suffice to say that the regularization strength is not affecting feature selection very much, which I was surprised by. I'm also surprised by the fact that the model *is* discriminating based on age, even when it is illegal to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.045621089376376468"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.corr().ix['NumberOfDependents', 'SeriousDlqin2yrs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Number of dependents and serious delinquency in 2 years is slightly positively correlated. I think that someone with more dependents is more likely to have trouble paying off their loans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B\n",
    "\n",
    "Look at your best models (in terms of LB AUC). Try to perform feature interpretability for them. Are the results consistent with interpreting a simple logistic regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# XGB, Raymond Wen's parameters\n",
    "# Raymond Wen's parameters\n",
    "params = {\n",
    "    'n_estimators': 1000,\n",
    "    'eta': 0.01,\n",
    "    'max_depth': 4,\n",
    "    'min_child_weight': 5,\n",
    "    'subsample': 0.4,\n",
    "    'gamma': 0.8,\n",
    "    'colsample_bytree': 0.4,\n",
    "    'lambda': 0.93,\n",
    "    'alpha': 0.5,\n",
    "    'eval_metric': 'auc',\n",
    "    'objective': 'binary:logistic',\n",
    "    # Increase this number if you have more cores.\n",
    "    # Otherwise, remove it and it will default\n",
    "    # to the maxium number.\n",
    "    'nthread': 4,\n",
    "    'booster': 'gbtree',\n",
    "    'tree_method': 'exact',\n",
    "    'silent': 1,\n",
    "    'seed': SEED\n",
    "}\n",
    "\n",
    "xg_booster = xgb.train(params, dtrain, num_boost_round)\n",
    "preds = xg_booster.predict(dtest)\n",
    "write_preds('submissions/xgb_raymond_{}.csv'.format(SEED), preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f0': 2640,\n",
       " 'f1': 1671,\n",
       " 'f2': 1060,\n",
       " 'f3': 2627,\n",
       " 'f4': 1966,\n",
       " 'f5': 1467,\n",
       " 'f6': 959,\n",
       " 'f7': 975,\n",
       " 'f8': 777,\n",
       " 'f9': 578}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_booster.get_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the results are different than performing a simple logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "The Age Discrimination in Employment Act (ADEA) forbids age discrimination against people who are age 40 or older, see \n",
    "https://www.eeoc.gov/laws/types/age.cfm\n",
    "\n",
    "Are your models considering age as a factor of influence?\n",
    "\n",
    "Fit a model for people over 40 or 50 and a model for younger people. Are the two models different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/cs-training.csv', index_col=0)\n",
    "test = pd.read_csv('data/cs-test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop dependent variable in test\n",
    "test = test.drop(['SeriousDlqin2yrs'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fill missing with mean\n",
    "train = train.fillna(train.mean())\n",
    "test = test.fillna(test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split on age\n",
    "train_young = train[train.age <= 40]\n",
    "train_old = train[train.age > 40]\n",
    "\n",
    "test_young = test[test.age <= 40]\n",
    "test_old = test[test.age > 40]\n",
    "\n",
    "# Seperate dependent and independent\n",
    "X_train_young = train_young.drop(['SeriousDlqin2yrs'], axis=1)\n",
    "y_train_young = train_young['SeriousDlqin2yrs']\n",
    "\n",
    "X_train_old = train_old.drop(['SeriousDlqin2yrs'], axis=1)\n",
    "y_train_old = train_old['SeriousDlqin2yrs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.79526594  0.80453694  0.80701095  0.81433514  0.80898215  0.79669316\n",
      "  0.81571334  0.82097823  0.8071783   0.81165422]\n",
      "0.808234837985\n"
     ]
    }
   ],
   "source": [
    "# Young model\n",
    "xg = xgb.XGBClassifier(max_depth=8, learning_rate=0.3, n_estimators=155,\n",
    "                       min_child_weight=0.6, subsample=1.0, colsample_bytree=0.45)\n",
    "\n",
    "score = cross_val_score(xg, X=X_train_young, y=y_train_young, scoring='roc_auc', cv=10, n_jobs=-1)\n",
    "print(score)\n",
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.84046547  0.85704543  0.8401881   0.84075806  0.84354391  0.84919176\n",
      "  0.84959688  0.84603375  0.85213222  0.86746431]\n",
      "0.848641989413\n"
     ]
    }
   ],
   "source": [
    "# Old model\n",
    "xg = xgb.XGBClassifier(max_depth=8, learning_rate=0.3, n_estimators=155,\n",
    "                       min_child_weight=0.6, subsample=1.0, colsample_bytree=0.45)\n",
    "\n",
    "score = cross_val_score(xg, X=X_train_old, y=y_train_old, scoring='roc_auc', cv=10, n_jobs=-1)\n",
    "print(score)\n",
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "We can see that a model with the same parameters does much better on the set of older people than on the set of younger people. Age is clearly an influence factor in this dataset. We can use a RandomizedSearch to see if different parameters are selected for models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_cv(model, name):\n",
    "    print(\"Best parameter set found on {} model:\\n\".format(name))\n",
    "    print(model.best_params_)\n",
    "    print()\n",
    "    for params, mean_score, scores in model.grid_scores_:\n",
    "        print(\"{0:.3f} (+/-{1:.03f}) for {2}\".format(mean_score, scores.std() * 2, params))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set found on young model:\n",
      "\n",
      "{'min_child_weight': 1.0259783520851542, 'learning_rate': 0.01, 'colsample_bytree': 0.5, 'max_depth': 8, 'n_estimators': 200}\n",
      "\n",
      "0.832 (+/-0.006) for {'min_child_weight': 1.0259783520851542, 'learning_rate': 0.1, 'colsample_bytree': 0.4, 'max_depth': 6, 'n_estimators': 200}\n",
      "0.825 (+/-0.007) for {'min_child_weight': 1.0259783520851542, 'learning_rate': 0.1, 'colsample_bytree': 0.4, 'max_depth': 8, 'n_estimators': 200}\n",
      "0.837 (+/-0.008) for {'min_child_weight': 1.0259783520851542, 'learning_rate': 0.01, 'colsample_bytree': 0.5, 'max_depth': 8, 'n_estimators': 200}\n",
      "0.825 (+/-0.008) for {'min_child_weight': 1.0259783520851542, 'learning_rate': 0.1, 'colsample_bytree': 0.5, 'max_depth': 8, 'n_estimators': 200}\n",
      "0.831 (+/-0.006) for {'min_child_weight': 1.0259783520851542, 'learning_rate': 0.1, 'colsample_bytree': 0.5, 'max_depth': 6, 'n_estimators': 200}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aetherzephyr/anaconda3/envs/datasci/lib/python3.5/site-packages/sklearn/model_selection/_search.py:662: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set found on old model:\n",
      "\n",
      "{'min_child_weight': 1.0259783520851542, 'learning_rate': 0.01, 'colsample_bytree': 0.5, 'max_depth': 8, 'n_estimators': 200}\n",
      "\n",
      "0.869 (+/-0.009) for {'min_child_weight': 1.0259783520851542, 'learning_rate': 0.01, 'colsample_bytree': 0.5, 'max_depth': 8, 'n_estimators': 200}\n",
      "0.863 (+/-0.010) for {'min_child_weight': 1.0259783520851542, 'learning_rate': 0.1, 'colsample_bytree': 0.4, 'max_depth': 8, 'n_estimators': 200}\n",
      "0.868 (+/-0.009) for {'min_child_weight': 1.0259783520851542, 'learning_rate': 0.01, 'colsample_bytree': 0.4, 'max_depth': 6, 'n_estimators': 200}\n",
      "0.868 (+/-0.009) for {'min_child_weight': 1.0259783520851542, 'learning_rate': 0.01, 'colsample_bytree': 0.4, 'max_depth': 8, 'n_estimators': 200}\n",
      "0.863 (+/-0.010) for {'min_child_weight': 1.0259783520851542, 'learning_rate': 0.1, 'colsample_bytree': 0.5, 'max_depth': 8, 'n_estimators': 200}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aetherzephyr/anaconda3/envs/datasci/lib/python3.5/site-packages/sklearn/model_selection/_search.py:662: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "parameters = {\n",
    "    'max_depth': [6, 8],\n",
    "    'learning_rate': [0.1, 0.01],\n",
    "    'n_estimators': [200],\n",
    "    'min_child_weight': [1/(0.95**(1/2))],\n",
    "    'colsample_bytree': [0.4, 0.5]\n",
    "}\n",
    "\n",
    "xg_clf = RandomizedSearchCV(xgb.XGBClassifier(), parameters, n_iter=5, cv=5, n_jobs=-1, scoring='roc_auc')\n",
    "xg_clf.fit(X_train_young, y_train_young)\n",
    "print_cv(xg_clf, 'young')\n",
    "\n",
    "xg_clf = RandomizedSearchCV(xgb.XGBClassifier(), parameters, n_iter=5, cv=5, n_jobs=-1, scoring='roc_auc')\n",
    "xg_clf.fit(X_train_old, y_train_old)\n",
    "print_cv(xg_clf, 'old')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Again, we see that when seperated by age, the model with older people performs much better. Also, different paramters are selected. In the younger model, `max_depth` was chosen to be 6, while the older model chose `max_depth` as 8. If we had searched over more parameter values, the models would likely be completely different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B\n",
    "\n",
    "As a law-maker do you think that forcing age and number of dependents to be forbidden features is a good idea for this problem? Try to base your discussion on what you discover from the data.\n",
    "\n",
    "## Answer\n",
    "\n",
    "I think that from a law point of view, those features should be forbidden no matter what the data says. Age should not be considered when deciding if a person can get a loan or not, because that does classify as age discrimination. Also, if we are to not discriminate for people 40 or older, we should not discriminate based on any age value.\n",
    "\n",
    "According to the data, knowing the age can be valuable in predicting financial distress. This is clear from the work we did in part A. Since there is such a boost in performace for a model predicting on only people over the age of 40, this means that using their age is very helpful to the model. This may seem like a good idea since we get a better AUC ROC score, but in fact this is leading to age discrimination. As a law-maker, I would not feel comfortable knowing that we can predict so much better for people over age 40. This may be generalization and can lead to discrimination based on a person's age.\n",
    "\n",
    "Because of this, I think that it would be a good idea (as a law-maker) to make age and the number of dependents to be forbidden features. However, from a data perspective (disregarding law), knowing the age can help your models a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
