{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE 379K Lab 4\n",
    "\n",
    "## Rohan Nagar and Wenyang Fu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sympy\n",
    "import matplotlib as mpl\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Linear Algebra in Python\n",
    "\n",
    "## Part 1\n",
    "\n",
    "Consider the linear subspace $S = span\\{v_1, v_2, v_3, v_4\\}$ where $v_1 = [1, 2, 3, 4]$, $v_2 = [0, 1, 0, 1]$, $v_3 = [1, 4, 3, 6]$, $v_4 = [2, 11, 6, 15]$. Create a vector inside $S$ different from $v_1, v_2, v_3, v_4$. Create a vector not in $S$. How would you check if a new vector is in $S$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Matrix([\n",
       " [1.0,   0, 3.0, 2.0],\n",
       " [  0, 1.0,   0, 1.0],\n",
       " [  0,   0,   0,   0],\n",
       " [  0,   0,   0,   0]]), [0, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = np.array([1, 2, 3, 4])\n",
    "v2 = np.array([0, 1, 0, 1])\n",
    "v3 = np.array([1, 4, 3, 6])\n",
    "v4 = np.array([2, 11, 6, 15])\n",
    "mat = np.vstack((v1, v2,v3, v4))\n",
    "reduced = sympy.Matrix(mat).rref()\n",
    "reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector is contained in $S$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Matrix([\n",
       " [1.0,   0, 3.0, 2.0],\n",
       " [  0, 1.0,   0, 1.0],\n",
       " [  0,   0,   0,   0],\n",
       " [  0,   0,   0,   0],\n",
       " [  0,   0,   0,   0]]), [0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_in_s = np.array([2, 4, 6, 8])\n",
    "mat1 = np.vstack((mat, vec_in_s))\n",
    "reduced = sympy.Matrix(mat1).rref()\n",
    "reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector not contained in $S$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Matrix([\n",
       " [1.0,   0, 3.0,   0],\n",
       " [  0, 1.0,   0,   0],\n",
       " [  0,   0,   0, 1.0],\n",
       " [  0,   0,   0,   0],\n",
       " [  0,   0,   0,   0]]), [0, 1, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_not_in_s = np.array([1, 1, 3, 4])\n",
    "mat2 = np.vstack((mat, vec_not_in_s))\n",
    "reduced = sympy.Matrix(mat2).rref()\n",
    "reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How would you check if a new vector is in $S$?\n",
    "\n",
    "If a new vector is in $S$, then it must be expressable as a linear combination of the existing vectors in $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Find the dimension of the subspace $S$.\n",
    "\n",
    "A: The dimension of the subspace is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  3.  2.]\n",
      " [ 0.  1.  0.  1.]]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "mat = np.vstack((v1, v2,v3, v4))\n",
    "reduced = sympy.Matrix(mat).rref()\n",
    "s = np.array(reduced[0].tolist())  # Convert sympy matrix back to np.ndarray\n",
    "s = s[~np.all(s == 0, axis=1)] # Drop non-linearly independent vectors\n",
    "s = s.astype(np.float64) # Typecast sympy floats back into numpy floats\n",
    "print(s)\n",
    "print(s.shape[0])\n",
    "rank = s.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "Find an orthonormal basis for the subspace S.\n",
    "\n",
    "A: The QR decomposition provides an easy way to find an orthornomal basis for the subspace S. The first $n$ columns of $Q$ (where $n = rank(span(S))$) constitute an orthonormal basis of $S$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.18257419  0.2236068   0.82454859  0.48660692]\n",
      " [-0.36514837 -0.67082039  0.40003314 -0.50659664]\n",
      " [-0.54772256  0.67082039 -0.00816077 -0.4999334 ]\n",
      " [-0.73029674 -0.2236068  -0.40003314  0.50659664]]\n",
      "[[ -5.47722558e+00  -1.09544512e+00  -7.66811581e+00  -1.86225670e+01]\n",
      " [  0.00000000e+00  -8.94427191e-01  -1.78885438e+00  -6.26099034e+00]\n",
      " [  0.00000000e+00   0.00000000e+00  -1.02375271e-16  -1.07707511e-15]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00  -6.72908534e-16]]\n",
      "[[-0.18257419  0.2236068 ]\n",
      " [-0.36514837 -0.67082039]\n",
      " [-0.54772256  0.67082039]\n",
      " [-0.73029674 -0.2236068 ]]\n"
     ]
    }
   ],
   "source": [
    "# Help from here:\n",
    "# https://stackoverflow.com/questions/27176453/scipy-find-bases-of-column-space-of-matrix\n",
    "\n",
    "q,r = np.linalg.qr(mat.T) # Transpose matrix to find column basis as opposed to row basis\n",
    "print(q)\n",
    "print(r)\n",
    "orthonormall_basis = q[:, :rank]\n",
    "print(orthogonal_basis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4\n",
    "\n",
    "Solve the optimization problem $min_{x \\in S}\\| x - z^* \\|_2$ where $z^* = [1, 0, 0, 0]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: PCA\n",
    "\n",
    "## Part 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Covariance matrices\n",
    "cov1 = [[0.5, 0, 0],\n",
    "        [0, 0.5, 0],\n",
    "        [0, 0, 0.7]]\n",
    "cov2 = [[0.5, 0, 0],\n",
    "        [0, 0.5, 0],\n",
    "        [0, 0, 0.01]]\n",
    "\n",
    "# Generate the samples\n",
    "label1_samples = np.random.multivariate_normal([0, 0, 0], cov1, 20)\n",
    "label2_samples = np.random.multivariate_normal([1, 1, 1], cov2, 20)\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(label1_samples.T[0], label1_samples.T[1], label1_samples.T[2], label='Label 1')\n",
    "ax.scatter(label2_samples.T[0], label2_samples.T[1], label2_samples.T[2], c='r', label='Label 2')\n",
    "ax.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "What do the points look like?\n",
    "\n",
    "#### The label 1 points are labled in blue above. These points are more spread out and they go much lower in the $z$ direction than the label 2 points. It is also generally lower in value in the $x$ direction than the label 2 points. The label 2 points are more compact and closer together. They form more of an ellipsoid shape than the label 1 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def covariance(X, Y):\n",
    "    ''' \n",
    "    Finds the covariance of the two arrays X and Y.\n",
    "    X and Y must be the same length.\n",
    "    '''\n",
    "    if len(X) is not len(Y):\n",
    "        return None\n",
    "    \n",
    "    X_mean = np.mean(X)\n",
    "    Y_mean = np.mean(Y)\n",
    "    \n",
    "    total = 0\n",
    "    for x, y in zip(X, Y):\n",
    "        total += (x-X_mean)*(y-Y_mean)\n",
    "        \n",
    "    return total / (len(X) - 1)\n",
    "\n",
    "# Concatenate\n",
    "samples = label1_samples + label2_samples\n",
    "\n",
    "# Calculate the covariance matrix\n",
    "cov = [[covariance(col, col2) for col2 in samples.T] for col in samples.T]\n",
    "print(np.array(cov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def largest_eigenvectors(cov):\n",
    "    values, vectors = np.linalg.eig(cov)\n",
    "    \n",
    "    largest = max(values)\n",
    "    second_largest = max(np.delete(values, np.nonzero(values==largest)[0][0]))\n",
    "\n",
    "    first_vector = vectors[:,np.nonzero(values==largest)[0][0]]\n",
    "    second_vector = vectors[:,np.nonzero(values==second_largest)[0][0]]\n",
    "    \n",
    "    return first_vector, second_vector\n",
    "    \n",
    "eigenvectors = largest_eigenvectors(cov)\n",
    "\n",
    "# TODO: project label1_samples and label2_samples onto the two eigenvectors and plot with labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did PCA make it easier to distingush the two labels in two dimensions?\n",
    "\n",
    "#### TODO: Answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Low Rank Approximation\n",
    "\n",
    "## Part 1\n",
    "\n",
    "Load the Mona Lisa image (in grayscale) and treat it as a matrix M. Perform a SVD on this matrix using $\\tt{linalg.svd}$. You can perform a low-rank approximation by zeroing out singular values and keeping only the top $k$. Show the best rank for $k = 2$, $k = 5$, and $k = 10$ approximation to Mona Lisa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the image\n",
    "M = mpimg.imread('mona_lisa.png')\n",
    "plt.imshow(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svd = np.linalg.svd(M)\n",
    "\n",
    "# TODO: low-rank approxmiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "If each pixel is represented by two bytes, how many bits is your compressed Mona Lisa for each of those $k$ rank approximations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Kaggle\n",
    "\n",
    "## Part 1\n",
    "\n",
    "Make an account on Kaggle and find https://www.kaggle.com/c/house-prices-advanced-regression-techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "\n",
    "Follow the data preprocessing steps from https://www.kaggle.com/apapiu/house-prices-advanced-regression-techniques/regularized-linear-models. Then run a ridge regression using $\\alpha = 0.1$. Make a submission of this prediciton, what is the RSME you get? (Hint: remember to exponentiate $\\tt{np.expm1(ypred)}$ your predicitons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import skew\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n",
    "                      test.loc[:,'MSSubClass':'SaleCondition']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "prices = pd.DataFrame({\"price\":train[\"SalePrice\"], \"log(price + 1)\":np.log1p(train[\"SalePrice\"])})\n",
    "prices.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#log transform the target:\n",
    "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "\n",
    "#log transform skewed numeric features:\n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "\n",
    "skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n",
    "skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
    "skewed_feats = skewed_feats.index\n",
    "\n",
    "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
    "\n",
    "all_data = pd.get_dummies(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filling NA's with the mean of the column:\n",
    "all_data = all_data.fillna(all_data.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating matrices for sklearn:\n",
    "X_train = all_data[:train.shape[0]]\n",
    "X_test = all_data[train.shape[0]:]\n",
    "y = train.SalePrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run our Ridge model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = Ridge(alpha=0.1).fit(X_train, y)\n",
    "preds = np.expm1(model.predict(X_test))\n",
    "\n",
    "solution = pd.DataFrame({\"id\":test.Id, \"SalePrice\":preds})\n",
    "solution.to_csv(\"solution.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $\\alpha = 0.1$, the Ridge predictions submitted on Kaggle gave a score of $0.13029$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "Try to build the best model you can. Report the best RSME you got on the Kaggle wall and how you got it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
