{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE 379K: Lab 6\n",
    "\n",
    "## Rohan Nagar and Wenyang Fu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: K-Means\n",
    "\n",
    "### Part 1\n",
    "\n",
    "Implement the K-Means algorithm with a random initalization. Recall that there are two steps: given the centers, computing the clusters, and given the cluseters, computing the centers. As discussed in class, the second is straightforward. Describe how you do the first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "Now implement K-Means++, i.e., the K-Means++ initalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "Download the data `iris_data.csv` from Canvas. Plot Sepal Width vs. Sepal Length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "\n",
    "Use your K-Means++ algorithm to cluster the above two variables into 2, 3, 4, and 5 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Spectral Clustering\n",
    "\n",
    "Spectral clustering, as described in class, is a powerful and much-used algorithm. A very in-depth tutorial can be found here: http://www.cs.cmu.edu/~aarti/Class/10701/readings/Luxburg06_TR.pdf. In class we discussed the application of spectral clustering to a given graph. But we can also apply it to other data sets, by turning them into weighted graphs. This can be done in several ways, but roughly speaking, we convert by means of a similarity function: two nodes have an edge between them with weight equal to their similiarity. Read section 2 of the above tutorial on different ways to create the similiarity graph. Then read section 4 on the different graph clustering algorithms, especically focusing on the one by Ng, Jordan and Weiss (2002). This is the one we discussed in class.\n",
    "\n",
    "### Part 1\n",
    "\n",
    "Implement the Ng, Jordan and Weiss spectral clustering algorithm. You may use basic linear algebra functions, including SVD and eigenvalue decomposition, but do not use `sklearn.cluster.spectral.clustering` or anything similar. Of course, you can use your K-Means algorithm from above for the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "Write a program that generates N data points uniformly at random on two concentric rings, and then add two-dimensional Gaussian noise of variance $\\sigma_1^2$ and $\\sigma_2^2$ to each point. There are four parameters of interest here: the number of points, the difference in the radii of the two rings, the amount (the variance) of noise added to the internal ring, and the amount of noise added to the external ring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "Experiement with the 3 methods of creating similarity functions described in section 2 of the paper. Pick one that you like (clearly state which one), and fixing $N = 500$ and $\\sigma_1 = \\sigma_2 = 1$, and also the radius of the inner ring to be $1$, find the range of the outer ring radius for which spectral clustering, using your chosen similarity function, succeeds in clustering the two different rings. Note that you will be using $k = 2$ in your spectral clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Spectral Clustering vs. K-Means\n",
    "\n",
    "### Part 1\n",
    "\n",
    "Load the data from the two-dimenstional `two_moons.csv` and the three-dimensional `linked_chains.csv`. Each row represents a point. Note that the first column gives the ground truth - the cluster identity of each point - and so can be ignored. Plot each in 2d and 3d respectively, so you can see what is going on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "Run K-Means on both. How do you do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "Find the best similarity function you can, and run Spectral Clustering. Which is better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: More Spectral Clustering vs. K-Means\n",
    "\n",
    "### Part 1\n",
    "\n",
    "Generate an example where K-Means does better than Spectral Clustering. Explain how you constructed it and why, in addition to showing the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5: Computational Complexity and Spectral Clustering\n",
    "\n",
    "### Part 1\n",
    "\n",
    "This is a more open ended question. Computing the eigenvalues and eigenvectors of a matrix is not easy, and the complexity depends on the size of the matrix among other factors. An important factor is the sparsity of the matrix. Which aspects of your algorithm for spectral clustering could affect this? Using one of the examples from above, or another example of your choosing, try to illustrate this pheonomeon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
